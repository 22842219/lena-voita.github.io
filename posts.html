---
layout: default
title: Blog
menu: yes
order: 1
---

<hr>


<!-- <span style="color:darkblue;font-size:28px;">The Story of Heads </span> -->
<h1 style="font-size:28px;">Evolution of Representations in the Transformer</h1>


<a class="float-right">
    <img src="../resources/posts/emnlp19_evolution/fugue_logo_on_white-min-min.png" alt="" style="max-width:350px; height:auto; float: right"/>
</a>


<span style="font-size:14px;">
This is a post for the EMNLP 2019 paper
    <a href="https://arxiv.org/abs/1909.01380">
            The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives.
    </a>
</span>


<br/>
<br/>
<span style="font-size:15px;">
We look at the evolution of representations of individual tokens in Transformers trained with different
    training objectives (MT, LM, MLM - BERT-style) from the
    <a href="https://www.cs.huji.ac.il/labs/learning/Papers/allerton.pdf">Information Bottleneck</a>
    perspective and show, that:
<ul>
  <li>LMs gradually forget past when forming future;</li>
  <li>for MLMs, the evolution has the two stages of
      <font face="symbol">context encoding</font> and <font face="symbol">token reconstruction</font>;</li>
    <li>MT representations get refined with context,
        but less processing is happening.</li>
</ul>
</span>


<a class="pull-right" href="/posts/emnlp19_evolution.html" onMouseOver="document.readmore3.src='../resources/posts/buttons/button_read_more_push-min.png';" onMouseOut="document.readmore3.src='../resources/posts/buttons/button_read_more-min.png';">
<img src="../resources/posts/buttons/button_read_more-min.png" name="readmore3" width=120px class="pull-right"></a>
<a class="pull-right" href="https://arxiv.org/abs/1909.01380" onMouseOver="document.readpaper3.src='../resources/posts/buttons/button_read_paper_push-min.png';" onMouseOut="document.readpaper3.src='../resources/posts/buttons/button_read_paper-min.png';">
<img src="../resources/posts/buttons/button_read_paper-min.png" name="readpaper3" width=120px class="pull-right"></a>
<span style="font-size:15px; text-align: right; float: right; color:gray">September 2019</span>

<p></p>
<hr>

<!-- <span style="color:darkblue;font-size:28px;">The Story of Heads </span> -->
<h1 style="font-size:28px;">When a Good Translation is Wrong in Context</h1>


<video width="380" height="auto" style="float: right" loop autoplay muted>
  <source src="../resources/posts/acl19_ctx/cadec_post_crop.mp4" type="video/mp4">
</video>


<span style="font-size:14px;">
This is a post for the ACL 2019 paper
    <a href="https://www.aclweb.org/anthology/P19-1116">
            When a Good Translation is Wrong in Context: Context-Aware Machine Translation Improves on Deixis, Ellipsis, and Lexical Cohesion.
    </a>
</span>


<br/>
<br/>
<span style="font-size:15px;">
From this post, you will learn:
<ul>
  <li>which phenomena cause context-agnostic translations to be inconsistent with each other</li>
  <li>how we create test sets addressing the most frequent phenomena</li>
    <li>about a novel set-up for context-aware NMT with a large amount of sentence-level data and
        much less of document-level data</li>
  <li>about a new model for this set-up (<font color="#CA6F1E">C</font>ontext-<font color="#CA6F1E">A</font>ware
      <font color="#CA6F1E">Dec</font>oder, aka <font color="#CA6F1E">CADec</font>) - a two-pass MT model which first produces a draft translation of the current sentence, then corrects it using context.</li>
</ul>
</span>


<a class="pull-right" href="/posts/acl19_context.html" onMouseOver="document.readmore2.src='../resources/posts/buttons/button_read_more_push-min.png';" onMouseOut="document.readmore2.src='../resources/posts/buttons/button_read_more-min.png';">
<img src="../resources/posts/buttons/button_read_more-min.png" name="readmore2" width=120px class="pull-right"></a>
<a class="pull-right" href="https://www.aclweb.org/anthology/P19-1116" onMouseOver="document.readpaper2.src='../resources/posts/buttons/button_read_paper_push-min.png';" onMouseOut="document.readpaper2.src='../resources/posts/buttons/button_read_paper-min.png';">
<img src="../resources/posts/buttons/button_read_paper-min.png" name="readpaper2" width=120px class="pull-right"></a>
<a class="pull-right" href="https://github.com/lena-voita/good-translation-wrong-in-context" onMouseOver="document.viewcode2.src='../resources/posts/buttons/button_view_code_push-min.png';" onMouseOut="document.viewcode2.src='../resources/posts/buttons/button_view_code-min.png';">
<img src="../resources/posts/buttons/button_view_code-min.png" name="viewcode2" width=120px></a>
<span style="font-size:15px; text-align: right; float: right; color:gray">July 2019</span>

<p></p>
<hr>


<!-- <span style="color:darkblue;font-size:28px;">The Story of Heads </span> -->
<h1 style="font-size:28px;">The Story of Heads</h1>


<a class="float-right">
    <img src="../img/paper/acl19_heads-min.png" alt="" style="max-width:350px; height:auto; float: right"/>
</a>

<span style="font-size:14px;">
This is a post for the ACL 2019 paper
    <a href="https://www.aclweb.org/anthology/P19-1580">
            Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned.
    </a>
</span>

<br/>
<br/>
<span style="font-size:15px;">
From this post, you will learn:
<ul>
  <li>how we evaluate the importance of attention heads in Transformer</li>
  <li>which functions the most important encoder heads perform</li>
  <li>how we prune the vast majority of attention heads in Transformer without seriously affecting quality</li>
    <li>which types of model attention are most sensitive to the number of attention heads and on which layers </li>
</ul>
</span>


<a class="pull-right" href="/posts/acl19_heads.html" onMouseOver="document.readmore.src='../resources/posts/buttons/button_read_more_push-min.png';" onMouseOut="document.readmore.src='../resources/posts/buttons/button_read_more-min.png';">
<img src="../resources/posts/buttons/button_read_more-min.png" name="readmore" width=120px class="pull-right"></a>
<a class="pull-right" href="https://www.aclweb.org/anthology/P19-1580" onMouseOver="document.readpaper.src='../resources/posts/buttons/button_read_paper_push-min.png';" onMouseOut="document.readpaper.src='../resources/posts/buttons/button_read_paper-min.png';">
<img src="../resources/posts/buttons/button_read_paper-min.png" name="readpaper" width=120px class="pull-right"></a>
<a class="pull-right" href="https://github.com/lena-voita/the-story-of-heads" onMouseOver="document.viewcode.src='../resources/posts/buttons/button_view_code_push-min.png';" onMouseOut="document.viewcode.src='../resources/posts/buttons/button_view_code-min.png';">
<img src="../resources/posts/buttons/button_view_code-min.png" name="viewcode" width=120px></a>
<span style="font-size:15px; text-align: right; float: right; color:gray">June 2019</span>

<p></p>
<hr>